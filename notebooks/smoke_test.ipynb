{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Patient Summary Smoke Test (Colab Friendly)\n",
        "\n",
        "This notebook sets up an end-to-end sanity check for the summarization pipeline described in the paper. Instead of cloning GitHub from inside Colab, we copy the local repository (and datasets) into the Colab VM, install the light dependencies, and then run a single-example generation pass with each model family (LED, Llama 2, GPT-4). Use it whenever you need to confirm that the tooling still works after making local changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow\n",
        "\n",
        "1. Upload/sync this repository (including the preprocessed MIMIC data) to Google Drive ahead of time. I keep it under `MyDrive/patient_summaries_with_llms`, but you can change the path in the cell below.\n",
        "2. Open this notebook in Google Colab, switch to a GPU runtime, and run the cells sequentially.\n",
        "3. Each model section only touches a single JSONL example so we can focus on plumbing rather than performance. You'll still need the appropriate credentials/tokens for private Hugging Face models and GPT-4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Dec  4 04:07:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   28C    P0             43W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# @title (Optional) Verify GPU availability\n",
        "!nvidia-smi || echo \"GPU not visible; switch Colab runtime to GPU for Llama/LED runs.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3767457368.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import drive  # type: ignore\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1109454232.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mCOLAB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mREPO_IN_DRIVE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/Othercomputers/My Mac/cs598/patient_summaries_with_llms'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# @param {type:\"string\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mREPO_IN_DRIVE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title Sync the local repository from Google Drive (no git clone)\n",
        "from pathlib import Path\n",
        "import os\n",
        "import importlib.util\n",
        "\n",
        "COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
        "if COLAB:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    REPO_IN_DRIVE = Path('/content/drive/Othercomputers/My Mac/cs598/patient_summaries_with_llms')  # @param {type:\"string\"}\n",
        "    if not REPO_IN_DRIVE.exists():\n",
        "        raise FileNotFoundError(f\"Upload/sync the local repo to {REPO_IN_DRIVE} first.\")\n",
        "    TARGET_DIR = Path('/content/patient_summaries_with_llms')\n",
        "    TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    os.system(f\"rsync -a --delete '{REPO_IN_DRIVE}/' '{TARGET_DIR}/'\")\n",
        "    %cd /content/patient_summaries_with_llms\n",
        "else:\n",
        "    print(\"Running outside Colab; using the current working directory.\")\n",
        "    TARGET_DIR = Path.cwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Install Python dependencies (once per Colab VM)",
        "%pip install -q torch==2.1.2+cu118 torchvision==0.16.2+cu118 torchaudio==2.1.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118",
        "%pip install -q transformers==4.39.3 datasets==2.18.0 accelerate==0.28.0 peft==0.10.0 trl==0.7.10 sentencepiece evaluate rouge-score sacrebleu wandb guidance==0.1.10 pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Create single-example JSONL slices for all smoke tests\n",
        "import json\n",
        "import itertools\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_ROOT = Path('.')\n",
        "DATA_ROOT = REPO_ROOT / 'data' / 'ann-pt-summ' / '1.0.1' / 'mimic-iv-note-ext-di-bhc' / 'dataset'\n",
        "assert DATA_ROOT.exists(), f\"Missing data directory: {DATA_ROOT}\"\n",
        "\n",
        "SPLIT_FILENAME = 'valid.json'  # @param {type:\"string\"}\n",
        "NUM_EXAMPLES = 1  # @param {type:\"integer\"}\n",
        "PROMPT_EXAMPLES = 3  # @param {type:\"integer\"}\n",
        "\n",
        "split_path = DATA_ROOT / SPLIT_FILENAME\n",
        "SUBSET_DIR = Path('/content/data_subsets') if Path('/content').exists() else REPO_ROOT / 'data_subsets'\n",
        "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
        "SMOKE_JSONL = SUBSET_DIR / 'smoke_valid.jsonl'\n",
        "\n",
        "with split_path.open() as src, SMOKE_JSONL.open('w') as dst:\n",
        "    for line in itertools.islice(src, NUM_EXAMPLES):\n",
        "        dst.write(line)\n",
        "\n",
        "print(f\"Wrote {NUM_EXAMPLES} example(s) to {SMOKE_JSONL}\")\n",
        "\n",
        "GPT4_DATA_DIR = REPO_ROOT / 'gpt-4' / 'summarization_data'\n",
        "GPT4_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "IN_CONTEXT_FILE = GPT4_DATA_DIR / 'smoke_0_in-context.json'\n",
        "TEST_FILE = GPT4_DATA_DIR / 'smoke_0_test.json'\n",
        "\n",
        "train_path = DATA_ROOT / 'train.json'\n",
        "with train_path.open() as src, IN_CONTEXT_FILE.open('w') as dst:\n",
        "    for line in itertools.islice(src, PROMPT_EXAMPLES):\n",
        "        dst.write(line)\n",
        "\n",
        "shutil.copyfile(SMOKE_JSONL, TEST_FILE)\n",
        "print(f\"Prepared GPT-4 smoke files under {GPT4_DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Peek at the sample example\n",
        "import json\n",
        "with open(SMOKE_JSONL) as f:\n",
        "    example = json.loads(next(f))\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title LED-base: generate one summary via run_summarization.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "LED_OUTPUT = Path('/content/led_smoke_outputs') if Path('/content').exists() else Path('led_smoke_outputs')\n",
        "LED_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
        "os.environ['WANDB_MODE'] = 'offline'\n",
        "os.environ['WANDB_PROJECT'] = 'patient-summary-smoke'\n",
        "\n",
        "!python summarization/run_summarization.py     --model_name_or_path allenai/led-base-16384     --do_predict     --test_file {SMOKE_JSONL}     --text_column text     --summary_column summary     --output_dir {LED_OUTPUT}     --per_device_eval_batch_size 1     --predict_with_generate     --max_source_length 2048     --max_target_length 256     --max_test_samples 1     --report_to none"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Inspect LED prediction\n",
        "from pathlib import Path\n",
        "\n",
        "pred_file = LED_OUTPUT / 'generated_predictions.txt'\n",
        "print(pred_file.read_text() if pred_file.exists() else 'No prediction file produced.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Llama 2 7B (4-bit inference) on the same exampleimport osimport jsonimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerLLAMA_7B_MODEL = 'meta-llama/Llama-2-7b-chat-hf'  # @param {type:\"string\"}HF_TOKEN = os.environ.get('HF_TOKEN', None)bnb_config = BitsAndBytesConfig(    load_in_4bit=True,    bnb_4bit_compute_dtype=torch.float16,    bnb_4bit_quant_type='nf4',    bnb_4bit_use_double_quant=True,)tokenizer = AutoTokenizer.from_pretrained(LLAMA_7B_MODEL, use_fast=False, token=HF_TOKEN)if tokenizer.pad_token is None:    tokenizer.pad_token = tokenizer.eos_tokenmodel = AutoModelForCausalLM.from_pretrained(    LLAMA_7B_MODEL,    device_map='auto',    quantization_config=bnb_config,    torch_dtype=torch.float16,    token=HF_TOKEN,)model.eval()with open(SMOKE_JSONL) as f:    sample = json.loads(next(f))prompt = (\"You are a helpful clinician writing plain-language discharge instructions.\"          +    f\"Brief Hospital Course:{sample['text']}\" + \"Patient summary:\")inputs = tokenizer(prompt, return_tensors='pt').to(model.device)with torch.inference_mode():    generation = model.generate(**inputs, max_new_tokens=256, do_sample=False)result = tokenizer.decode(generation[0], skip_special_tokens=True)summary = result.split('Patient summary:')[-1].strip()print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title (Optional) Llama 2 70B hook \u2014 requires >=80GB GPURUN_LLAMA_70B = False  # @param {type:\"boolean\"}LLAMA_70B_MODEL = 'meta-llama/Llama-2-70b-chat-hf'  # @param {type:\"string\"}if RUN_LLAMA_70B:    import os    import json    import torch    from transformers import AutoModelForCausalLM, AutoTokenizer    HF_TOKEN = os.environ.get('HF_TOKEN', None)    bnb_config = BitsAndBytesConfig(        load_in_4bit=True,        bnb_4bit_compute_dtype=torch.float16,        bnb_4bit_quant_type='nf4',        bnb_4bit_use_double_quant=True,    )    tokenizer = AutoTokenizer.from_pretrained(LLAMA_70B_MODEL, use_fast=False, token=HF_TOKEN)    if tokenizer.pad_token is None:        tokenizer.pad_token = tokenizer.eos_token    model = AutoModelForCausalLM.from_pretrained(        LLAMA_70B_MODEL,        device_map='auto',        quantization_config=bnb_config,        torch_dtype=torch.float16,        token=HF_TOKEN,    )    model.eval()    with open(SMOKE_JSONL) as f:        sample = json.loads(next(f))    prompt = (        \"You are a helpful clinician writing plain-language discharge instructions.\"        f\"Brief Hospital Course:{sample['text']}\"        \"Patient summary:\"    )    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)    with torch.inference_mode():        generation = model.generate(**inputs, max_new_tokens=256, do_sample=False)    result = tokenizer.decode(generation[0], skip_special_tokens=True)    summary = result.split('Patient summary:')[-1].strip()    print(summary)else:    print('Skipping 70B run. Enable RUN_LLAMA_70B once you have enough GPU memory.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title GPT-4 single-example run (requires gpt-4/config.yaml with API credentials)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "GPT4_OUTPUT_DIR = Path('/content/gpt4_smoke_outputs') if Path('/content').exists() else Path('gpt4_smoke_outputs')\n",
        "GPT4_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "%cd gpt-4\n",
        "!python run_summarization.py --task_id 0 --prompt_id 3 --model_name gpt-4 --n_shot 0 --what_for smoke --save_path {GPT4_OUTPUT_DIR / 'gpt4_smoke.jsonl'} --verbose\n",
        "%cd -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Inspect GPT-4 output\n",
        "from pathlib import Path\n",
        "\n",
        "gpt4_results = GPT4_OUTPUT_DIR / 'gpt4_smoke.jsonl'\n",
        "if gpt4_results.exists():\n",
        "    with gpt4_results.open() as f:\n",
        "        print(f.read())\n",
        "else:\n",
        "    print('Run the GPT-4 cell after adding config.yaml with valid API keys.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "colab_led_smoke_test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}